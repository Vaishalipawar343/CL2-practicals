{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7de81d-2ca3-4801-b136-74f2ec70d917",
   "metadata": {},
   "source": [
    "IR pr 5\n",
    "\n",
    "Implement Page Rank Algorithm. (Use python or beautiful soup for implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f96fab-089c-4703-9030-b16318bbfb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def pagerank(G, alpha=0.85, personalization=None, \n",
    "             max_iter=100, tol=1.0e-6, weight='weight', \n",
    "             dangling=None, nstart=None):\n",
    "\n",
    "    if len(G) == 0:\n",
    "        return {}\n",
    "  \n",
    "    # Make sure graph is directed\n",
    "    if not G.is_directed():\n",
    "        D = G.to_directed()\n",
    "    else:\n",
    "        D = G\n",
    "        \n",
    "    # Convert to right stochastic form\n",
    "    W = nx.stochastic_graph(D, weight=weight)\n",
    "    N = W.number_of_nodes()\n",
    "    \n",
    "    # Choose starting vector if not given\n",
    "    if nstart is None:\n",
    "        x = dict.fromkeys(W, 1.0 / N)\n",
    "    else:\n",
    "        s = float(sum(nstart.values()))\n",
    "        x = dict((k, v / s) for k, v in nstart.items())\n",
    "        \n",
    "    # Assign uniform personalization vector if not given \n",
    "    if personalization is None:\n",
    "        p = dict.fromkeys(W, 1.0 / N)\n",
    "    else:\n",
    "        missing = set(G) - set(personalization)\n",
    "        if missing:\n",
    "            raise nx.NetworkXError(\"Personalization dictionary must have a value for every node\")\n",
    "        s = float(sum(personalization.values()))\n",
    "        p = dict((k, v / s) for k, v in personalization.items())\n",
    "        \n",
    "    # Handle dangling nodes\n",
    "    if dangling is None:\n",
    "        dangling_weights = p\n",
    "    else:\n",
    "        missing = set(G) - set(dangling)\n",
    "        if missing:\n",
    "            raise nx.NetworkXError(\"Dangling node dictionary must have a value for every node\")\n",
    "        s = float(sum(dangling.values()))\n",
    "        dangling_weights = dict((k, v / s) for k, v in dangling.items())\n",
    "        \n",
    "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "    \n",
    "    # Power iteration: iterate until convergence\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = dict.fromkeys(xlast.keys(), 0)\n",
    "        danglesum = alpha * sum(xlast[n] for n in dangling_nodes)\n",
    "        \n",
    "        for n in xlast:\n",
    "            for nbr in W[n]:\n",
    "                x[nbr] += alpha * xlast[n] * W[n][nbr][weight]\n",
    "        for n in x:\n",
    "            x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]\n",
    "  \n",
    "        # Check convergence\n",
    "        err = sum(abs(x[n] - xlast[n]) for n in x)\n",
    "        if err < N * tol:\n",
    "            return x\n",
    "\n",
    "    raise nx.NetworkXError(\"Pagerank: power iteration failed to converge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a2953f-a0ab-4109-80a5-09adb9d730fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.028140754356538296, 1: 0.01357121312059036, 2: 0.012364817594794228, 3: 0.012947394757077627, 4: 0.012579602095048716, 5: 0.013359273804006977, 6: 0.012563181397984126, 7: 0.013361023387481945, 8: 0.012954321686913451, 9: 0.012761511380706487, 10: 0.012743205133155102, 11: 0.013777822708937265, 12: 0.013189390633081333, 13: 0.013367693877617246, 14: 0.013165119415822713, 15: 0.012553868765815676, 16: 0.012971181359508801, 17: 0.012767041193923972, 18: 0.012171196605245443, 19: 0.012956528445964847, 20: 0.013376099455234458, 21: 0.01338072253901845, 22: 0.012970423672175957, 23: 0.01276638958778285, 24: 0.012964677584817496, 25: 0.013581017190426991, 26: 0.013777822708937265, 27: 0.012383392086373365, 28: 0.013582208698154427, 29: 0.012766694341643053, 30: 0.013152664215660073, 31: 0.012548794032799784, 32: 0.012581343542569118, 33: 0.012965366742511707, 34: 0.012354382770135987, 35: 0.011586199691226588, 36: 0.012774065834191774, 37: 0.012569282602096098, 38: 0.013152833619048183, 39: 0.012987803987632883, 40: 0.013571883501539771, 41: 0.012959700895443124, 42: 0.027753399705866065, 43: 0.027231417175943073, 44: 0.026962929117630013, 45: 0.026417626600270375, 46: 0.026075734778978676, 47: 0.02591792832074534, 48: 0.025284646934637604, 49: 0.02479170240061982, 50: 0.024666979747999043, 51: 0.02446957061183689, 52: 0.0240102564297208, 53: 0.02349748832394124, 54: 0.022970669658989945, 55: 0.022634057809725536, 56: 0.022652684401476747, 57: 0.02221001214572596, 58: 0.02164099319975149, 59: 0.021791991616507425}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a random Barabási–Albert graph\n",
    "G = nx.barabasi_albert_graph(60, 41) \n",
    "\n",
    "# Compute PageRank using NetworkX’s built-in function\n",
    "pr = nx.pagerank(G, alpha=0.4)\n",
    "\n",
    "print(pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f689e4-667f-4632-a7aa-8c42939b7e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08c9da-928b-4125-a1ae-8bff0528586d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
